import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
pip install tensorflow
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Flatten
# Load the dataset
df = pd.read_csv('spam_ham_dataset.csv')  # Replace 'spam_dataset.csv' with your dataset

# Clean the data (if needed)
# Remove missing values
df.dropna(inplace=True)

# Split the dataset into features (X) and labels (y)
X = df['text']
y = df['label']

# Convert labels to numerical values (0 for ham, 1 for spam)
y = y.map({'ham': 0, 'spam': 1})

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Use CountVectorizer to convert text data to numerical features
vectorizer = CountVectorizer()
X_train_vectorized = vectorizer.fit_transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)

# Build and train the Naive Bayes model
nb_model = MultinomialNB()
nb_model.fit(X_train_vectorized, y_train)
# Predict on the test set
y_pred = nb_model.predict(X_test_vectorized)

# Evaluate performance
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Confusion Matrix:\n{conf_matrix}')
print(f'Classification Report:\n{classification_rep}')
# Convert text data to numerical features using TensorFlow's TextVectorization layer
vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=10000, output_mode='int', output_sequence_length=100)
vectorize_layer.adapt(X_train.values)

model = Sequential([
    vectorize_layer,
    Embedding(input_dim=len(vectorize_layer.get_vocabulary()) + 1, output_dim=16, mask_zero=True),
    Flatten(),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))
# Evaluate the model on the test set
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Accuracy: {accuracy}')
# Step 7: Analyze model coefficients (for Naive Bayes)

# Access feature names from the CountVectorizer
feature_names = np.array(vectorizer.get_feature_names_out())

# Get the log probabilities of features for each class
log_probabilities = nb_model.feature_log_prob_

# Calculate the feature importance (positive for spam, negative for ham)
feature_importance = log_probabilities[1] - log_probabilities[0]

# Create a DataFrame to display feature names and coefficients
coef_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})

# Sort coefficients by absolute value to identify important features
coef_df['AbsoluteImportance'] = np.abs(coef_df['Importance'])
coef_df = coef_df.sort_values(by='AbsoluteImportance', ascending=False)

# Display the top features
print(coef_df.head(10))
